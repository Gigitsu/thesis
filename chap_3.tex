In questo capitolo parler\`o dei dataset utilizzati e di come \`e stato impostato l'esperimento.

\section{Part-of-Speech tagging}
In linguistica, il PoS tagging, \`e un processo che consisite nel raggruppare le parole di una frase in calssi dette, appunto, \emph{Part of Speech} o \emph{classi morfologiche}.
Questo raggruppamento, viene fatto sia in base alla definizione della parola stessa che al contesto in cui si trova.

Nella grammatica tradizionale, esistono un numero limitato di classi morfologiche (sostantivo, verbo, aggettivo, articolo, pronome, avverbo, congiunzione, preposizione, ecc..).

Ad esempio, possiamo classificare la frase \emph{Il cane abbaia.} in questo modo:

\centerline{Il\textbf{/ART} cane\textbf{/SOST} abbaia\textbf{/V} .\textbf{/PUNT}}

Tuttavia, chiaramente, esistono molte pi\`u classi di queste.
Per il pronome, possiamo trattare le forme singolari, plurali e possessive come classi distinte.
In molti linguaggi, inoltre, le parole possono essere distinte in base al loro ``caso'' o in base al genere.

Un esempio di classificazione pi\`u dettagliata pu\`o essere:

\begin{center}
Il\textbf{/ART:m:s}

cane\textbf{/SOST:m:s}

abbaia\textbf{/V:ind:pr:3:s}

.\textbf{/PUNT:sent}
\end{center}

In linguistica sono previste classi morfologiche per vari livelli di dettaglio, in base al modello di classificazione scelto.

Solitamente, in sistemi di PoS tagging computerizzati, vengono adottati modelli di classificazione che prevedono un numero elevato di classi (da 50 o pi\`u) e variano in base alla lingua adottata.
Esistono vari modelli comunemente accettati, quelli utilizzati per gli esperimenti di questa tesi sono:
\begin{itemize}
  \item Penn Treebank Tagset, composto da 42 classi e usato per la lingua inglese.
  \item Tanll Tagset, che conta fino a 328 classi ed \`e usato per la lingua italiana.
\end{itemize}

\section{Dataset utilizzati}
\nocite{Zanchetta:2005}
\nocite{Attardi:2008}

\subsection{CoNLL 2000}
CoNLL (\emph{Conference on Natural Language Learning}), \`e una conferenza organizzata annualmente dal gruppo \emph{SIGNLL}, a partire dal 1999.
Per ogni edizione della conferenza, sono state proposte delle attivit\`a condivise, ciascuna delle quali comprendeva dati di test e di training forniti direttamente dagli organizzatori.
In questo modo i partecipanti potevano essere valutati e confrontati in maniera sistematica.

Per gli esperimenti di questa tesi, sono stati usati i dataset di training e test forniti durante la quarta edizione della conferenza, tenutasi nel 2000.
I dati forniti consistono in file di testo dove ogni riga corrisponde ad una parola, mentre una riga vuota denota la fine di una frase.
Ciascuna riga \`e costituita, a sua volta, da 3 colonne:
\begin{itemize}
  \item la prima colonna contiene la parola corrente
  \item la seconda colonna la classe morfologica della parola
  \item la terza riga contiene un tag che indica la parte della frase a cui, la parola corrente, appartiene
\end{itemize}

Di seguito un esempio:

\begin{center}
  \begin{minipage}{5cm}
    \begin{verbatim}
     He        PRP  B-NP
     reckons   VBZ  B-VP
     the       DT   B-NP
     current   JJ   I-NP
     account   NN   I-NP
     deficit   NN   I-NP
     will      MD   B-VP
     narrow    VB   I-VP
     to        TO   B-PP
     only      RB   B-NP
     #         #    I-NP
     1.8       CD   I-NP
     billion   CD   I-NP
     in        IN   B-PP
     September NNP  B-NP
     .         .    O
    \end{verbatim}
  \end{minipage}
\end{center}

Di seguito la lista completa di classi del modello PennTreebank (Tabell~\ref{tab:penn-tagset}):

\begin{longtable}{| c | p{.40\textwidth} | p{.50\textwidth} |} \hline
  \thead{Tag} & \thead{Descrizione} & \thead{Esempio} \\ \hline
  CC & conjunction, coordinating & and, or, but \\ \hline
  CD & cardinal number & five, three, 13\% \\ \hline
  DT & determiner & the, a, these  \\ \hline
  EX & existential there & there were six boys  \\ \hline
  FW & foreign word & mais  \\ \hline
  IN & conjunction, subordinating or preposition & of, on, before, unless  \\ \hline
  JJ & adjective & nice, easy \\ \hline
  JJR & adjective, comparative & nicer, easier \\ \hline
  JJS & adjective, superlative & nicest, easiest  \\ \hline
  LS & list item marker &   \\ \hline
  MD & verb, modal auxillary & may, should  \\ \hline
  NN & noun, singular or mass & tiger, chair, laughter  \\ \hline
  NNS & noun, plural & tigers, chairs, insects  \\ \hline
  NNP & noun, proper singular & Germany, God, Alice  \\ \hline
  NNPS & noun, proper plural & we met two Christmases ago  \\ \hline
  PDT & predeterminer & both his children  \\ \hline
  POS & possessive ending & 's \\ \hline
  PRP & pronoun, personal & me, you, it  \\ \hline
  PRP\$ & pronoun, possessive & my, your, our  \\ \hline
  RB & adverb & extremely, loudly, hard   \\ \hline
  RBR & adverb, comparative & better  \\ \hline
  RBS & adverb, superlative & best  \\ \hline
  RP & adverb, particle & about, off, up  \\ \hline
  SYM & symbol & \%  \\ \hline
  TO & infinitival to & what to do?  \\ \hline
  UH & interjection & oh, oops, gosh  \\ \hline
  VB & verb, base form & think  \\ \hline
  VBZ & verb, 3rd person singular present & she thinks  \\ \hline
  VBP & verb, non-3rd person singular present & I think  \\ \hline
  VBD & verb, past tense & they thought  \\ \hline
  VBN & verb, past participle & a sunken ship  \\ \hline
  VBG & verb, gerund or present participle & thinking is fun  \\ \hline
  WDT & wh-determiner & which, whatever, whichever  \\ \hline
  WP & wh-pronoun, personal & what, who, whom  \\ \hline
  WP\$ & wh-pronoun, possessive & whose, whosever  \\ \hline
  WRB & wh-adverb & where, when  \\ \hline
  . & punctuation mark, sentence closer & .;?*  \\ \hline
  , & punctuation mark, comma & ,  \\ \hline
  : & punctuation mark, colon & :  \\ \hline
  ( & contextual separator, left paren & (  \\ \hline
  ) & contextual separator, right paren & ) \\ \hline
  \caption{Tagset Penn Treebank} \label{tab:penn-tagset}
\end{longtable}

\subsection{Evalita 2009}
\emph{Evalita} nasce grazie all'iniziativa dell'\emph{Italian Association for Computational Linguistics} (ALIC),
ed \`e stato approvato dall'\emph{Italian Association for Artificial Intelligence} (AI*IA)
e dall'\emph{Italian Association for Speech Science} (AISV).
Lo scopo del progetto \`e quello di promuovere lo sviluppo di tecnologie in ambito linguistico,
scritto e parlato, per la lingua italiana, fornendo un ambiente condiviso nel quale
differenti sistemi e approcci, possono essere sviluppati e valutati in maniera consistente.

La diffusione di attivit\`a e di metodologie di valutazione condivise costituisce un passo fondamentale
verso lo sviluppo di risorse e tecnologie di Natural Language Processing. Il buon riscontro
ottenuto da Evalita, sia in termini di partecipanti che in termini di qualit\`a dei risultati ottenuti,
ha dimostranto che vale la pena perseguire tali obiettivi anche per la lingua italiana.
Inoltre, i dati di test e di training per le attivit\`a proposte, vengono resi disponibili alla
comunit\`a scientifica come punto di riferimenti per futuri miglioramenti.

In particolare, per questa tesi, si far\`a riferimenti all'attivit\`a di
PoS-Tagging proposta da Evalita nel 2009.

I dataset forniti dagli organizzatori sono costituiti da articoli tratti dall'edizione
online del giornale \emph{La Repubblica} (http://www.repubblica.it).

L'intero corpus \`e formato da 108,874 parole divise in 3,719 frasi.

Questo \`e stato annotato in pi\`u passaggi: il primo \`e stato portato a termina dal
gruppo di Andrea Baroni, dell'Universit\`a di Bologna, che ha classificato manualmente
l'intero corpus adottando un modello di classificazione con poche classi; successivamente
\`e stato utilizzato \emph{MorphIt!}, uno strumento automatizzato, per assegnare una lista di
possibili classi morfologiche a ciascuna parola; il risultato \`e stato poi convertito, per mezzo
di uno script, nel modello di classificazione \emph{Tanl}.

Infine, l'intero corpus \`e stato revisionato manualmente.

\subsubsection{Formato dei dati}
Il dataset di training \`e formato da un unico file di testo, con codifica UTF-8,
dove ogni riga costituisce un token seguito dalla sua classe, separati da una tabulazione,
secondo il seguente schema:

\begin{center}
  \begin{minipage}{5cm}
    \begin{verbatim}
    <TOKEN_1> <TAG1>
    <TOKEN_2> <TAG2>
    ...
    <TOKEN_N> <TAGN>
    <RIGA VUOTA>
    \end{verbatim}
  \end{minipage}
\end{center}

Al termine di ogni frase \`e presenta una riga vuota. Ad esempio:

\begin{center}
  \begin{minipage}{5cm}
    \begin{verbatim}
      A             E
      ben           B
      pensarci      Vfc
      ,             FF
      l'            RDns
      intervista    Sfs
      dell'         EAns
      on.           SA
      Formica       SP
      e'            VAip3s
      stata         VApsfs
      accolta       Vpsfs
      in            E
      genere        Sms
      con           E
      disinteresse  Sms
      .             FS

    \end{verbatim}
  \end{minipage}
\end{center}

Nell'esempio precedente vengono mostrati alcuni fra i pi\`u comuni problemi di tokenizzazione:
\begin{itemize}
  \item Le abbreviazioni vengono trattate come token (\emph{on.})
  \item Possibili espressioni multi parola non vengono trattate come un unico token (\emph{in\_genere})
  \item I clitici non vengono separati dal token (\emph{pensarci})
\end{itemize}

\subsubsection{Il modello di classificazione Tanl}
Il modello \emph{Tanl} in base alle linee guida di \emph{EAGLES}, uno standard
comunemente accettato dalla comunit\`a NLP, ed \`e derivato dalla classificazione
morfologica adottata dal corpus ISST.

Tanl \`e composto da tre livelli di classi morfologiche, ciascun livello aggiunge maggior
dettaglio alla classificazione.

Il primo livello \`e composto da 14 classi (Tabella~\ref{tab:tanl-coarse}):

\begin{table}[H]
  \centering
  \begin{tabular}{| c || l |}
    \hline
    \thead{Tag} & \thead{Descrizione} \\
    \hline
    A & adjective \\
    B & adverb \\
    C & conjunction \\
    D & determiner \\
    E & preposition \\
    F & punctuation \\
    I & interjection \\
    N & numeral \\
    P & pronoun \\
    R & article \\
    S & noun \\
    T & predeterminer \\
    V & verb \\
    X & residual class \\ \hline
  \end{tabular}
  \caption{Tagset Tanl, primo livello} \label{tab:tanl-coarse}
\end{table}

Il secondo livello del modello Tanl contiene 36 classi, di seguito riportate, con relativi esempi (Tabella~\ref{tab:tanl-fine}):
\begin{longtable}{| c | p{.20\textwidth} | p{.30\textwidth} | p{.40\textwidth} |} \hline
  \thead{Tag} & \thead{Descrizione} & \thead{Esempio} & \thead{Contesto} \\ \hline
  A & adjective & bello, buono, pauroso, ottimo  & \parbox[t]{.40\textwidth}{una \emph{bella} passeggiata\\un \emph{ottimo} attaccante\\una persona \emph{paurosa}}\\ \hline
  AP & possessive adjective & mio, tuo, nostro, loro & \parbox[t]{.40\textwidth}{a \emph{mio} parere\\il \emph{tuo} libro}\\ \hline
  B & adverb & bene, fortemente, malissimo, & \parbox[t]{.40\textwidth}{arrivo \emph{domani}\\sto \emph{bene}}\\ \hline
  BN  & negation adverb & non & \emph{non} sto bene\\ \hline
  CC & coordinative conjunction & e, o, ma & \parbox[t]{.40\textwidth}{i libri \emph{e} i quaderni\\vengo \emph{ma} non rimango}\\ \hline
  CS & subordinative conjunction & mentre, quando & \parbox[t]{.40\textwidth}{\emph{quando} ho finito vengo\\\emph{mentre} scrivevo ho finito l'inchiostro}\\ \hline
  DD  & demonstrative determiner & questo, codesto, quello & \parbox[t]{.40\textwidth}{\emph{questo} denaro\\\emph{quella} famiglia}\\ \hline
  DE & exclamative determiner & che, quale, quanto & \parbox[t]{.40\textwidth}{\emph{che} disastro!\\\emph{quale} catastrofe!}\\ \hline
  DI & indefinite determiner & alcuno, certo, tale, parecchio, qualsiasi & \parbox[t]{.40\textwidth}{\emph{alcune} telefonate\\\emph{parecchi} giornali\\\emph{qualsiasi} persona}\\ \hline
  DQ & interrogative determiner & cui, quale & i \emph{cui} libri\\ \hline
  DR & relative determiner & che, quale, quanto & \parbox[t]{.40\textwidth}{\emph{che} cosa\\\emph{quanta} strada\\\emph{quale} formazione}\\ \hline
  E & preposition & di, a, da, in, su, attraverso, verso, prima\_di & \parbox[t]{.40\textwidth}{\emph{a} casa\\\emph{prima\_di} giorno\\\emph{verso} sera}\\ \hline
  EA & articulated preposition & alla, del, nei & \emph{nel} posto\\ \hline
  FB & balanced punctuation & ( ) [ ] { } - \_ ` & \emph{(}sempre\emph{)}\\ \hline
  FF & comma, hyphen & , - & carta, penna, 30\emph{-}40 persone\\ \hline
  FS & sentence boundary punctuation & . ? ! ... & cosa\emph{?}\\ \hline
  I & interjection & ahim\`e, beh, ecco, grazie & \emph{Beh}, che vuoi?\\ \hline
  N & cardinal number & uno, due, cento, mille, 28, 2000 & \parbox[t]{.40\textwidth}{\emph{due} partite\\\emph{28} anni}\\ \hline
  NO & ordinal number & primo, secondo, centesimo & \emph{secondo} posto\\ \hline
  PC & clitic pronoun &mi, ti, ci, si, te, ne, lo, la, gli & \parbox[t]{.40\textwidth}{me \emph{ne} vado\\\emph{si} sono rotti\\\emph{mi} lavo\\\emph{gli} parlo}\\ \hline
  PD & demonstrative pronoun & questo, quello, costui, ci\`o & \parbox[t]{.40\textwidth}{\emph{quello} di Roma\\\emph{costui} uccide}\\ \hline
  PE & personal pronoun & io, tu, egli, noi, voi & \parbox[t]{.40\textwidth}{\emph{io} parto\\\emph{noi} scriviamo}\\ \hline
  PI & indefinite pronoun & chiunque, ognuno, molto & \parbox[t]{.40\textwidth}{\emph{chiunque} venga\\i diritti di \emph{ognuno}}\\ \hline
  PP & possessive pronoun & mio, tuo, suo, loro, proprio & \parbox[t]{.40\textwidth}{il \emph{mio} \`e qui\\pi\`u bella della \emph{loro}}\\ \hline
  PQ & interrogative pronoun & che, chi, quanto & \parbox[t]{.40\textwidth}{non so \emph{chi} parta\\\emph{quanto} costa?\\\emph{che} ha fatto ieri?}\\ \hline
  PR & relative pronoun & che, cui, quale ci\`o & \parbox[t]{.40\textwidth}{\emph{che} dice\\il \emph{quale} afferma\\a \emph{cui} parlo}\\ \hline
  RD & determinative article & il, lo, la, i, gli, le & \parbox[t]{.40\textwidth}{\emph{il} libro\\\emph{i} gatti}\\ \hline
  RI & indeterminative article & uno, un, una & \parbox[t]{.40\textwidth}{\emph{un} amico\\\emph{una} bambina}\\ \hline
  S & common noun & amico, insegnante, verit\`a & \parbox[t]{.40\textwidth}{l'\emph{amico}\\la \emph{verit\`a}}\\ \hline
  SA & abbreviation & ndr, a.C., d.o.c., km & \parbox[t]{.40\textwidth}{30 \emph{km}\\sesto secolo \emph{a.C.}}\\ \hline
  SP & proper noun & Monica, Pisa, Fiat, Sardegna & \emph{Monica} scrive\\ \hline
  T & predeterminer & tutto, entrambi & \parbox[t]{.40\textwidth}{\emph{tutto} il giorno\\\emph{entrambi} i bambini}\\ \hline
  V & main verb & mangio, passato, camminando & \parbox[t]{.40\textwidth}{\emph{mangio} la sera\\il peggio \`e \emph{passato}\\ho \emph{scritto} una lettera}\\ \hline
  VA & auxiliary verb & avere, essere, venire & \parbox[t]{.40\textwidth}{il peggio \`e \emph{passato}\\ho \emph{scritto} una lettera\\viene \emph{fatto} domani}\\ \hline
  VM & modal verb & volere, potere, dovere, solere & \parbox[t]{.40\textwidth}{non posso \emph{venire}\\vuole \emph{comprare} il libro}\\ \hline
  X & residual class  & it includes formulae, unclassified words, alphabetic symbols and the like & \parbox[t]{.40\textwidth}{distanziare di \emph{43''}\\mi \emph{piacce}}\\ \hline
  \caption{Tagset Tanl, secondo livello} \label{tab:tanl-fine}
\end{longtable}

Nella forma pi\`u completa Tanl conta 328 classi che includono informazioni morfologiche, codificate in questo modo:
\begin{itemize}
  \item \emph{genere}: m (maschile), f (femminile), n (non specificato)
  \item \emph{numero}: s (singolare), p (plurale), n (non specificato)
  \item \emph{persona}: 1 (prima), 2 (seconda), 3 (terza)
  \item \emph{modo}: i (indicativo), m (imperativo), c (congiuntivo), d (condizionale), g (gerundio), f (infinito), p (participio)
  \item \emph{tempo}: p (presente), i (imperfetto), s (passato), f (futuro)
  \item \emph{clitico}: c segnala la presenza di clitici aggiuntivi
\end{itemize}

\section{Esperimenti}

Il codice per l'esperimento \`e stato scritto interamente in Lua, utilizza Torch come framework ed \`e stato eseguito in ambiente linux.

\subsection{Configurazione dell'ambiente}
Torch \`e stato installato nella cartella home, seguendo la documentazione ufficiale:

\lstinputlisting[language={bash}]{snippets/torch_install.sh}

Dopo aver installato Torch, \`e necessario installare alcuni pacchetti aggiuntivi per lua:

\lstinputlisting[language={bash}]{snippets/luarocks_packages.sh}

\subsubsection{luautf8}
Lua offre un supporto solo parziale alla codifica utf8.
Questo, nella maggior parte dei casi, non rappresenta un grosso problema.
Tuttavia, sviluppando un algoritmo che deve operare sui singoli caratteri di frasi scritte anche in lingua italiana, mi sono imbattuto in alcune limitazioni di Lua.

Prendiamo, ad esempio, il codice seguente

\lstinputlisting[language={[5.0]Lua}]{snippets/gmatch_english.lua}

La funzione \emph{gmatch} prende in input una espressione regolare e restituisce un iteratore che scorre tutta la stringa cercando tutte le possibili stringhe che combaciano con l'espressione regolare.
In questo caso l'espressione regolare \`e ``.'', quindi, l'iteratore restituito, trova tutti i singoli caratteri di cui \`e composta la stringa.
In altre parole, il codice precedente, equivale alla funzione \emph{split('')} comune in altri linguaggi ma assente in Lua.

Eseguendo questo codice in una REPL Lua, otteniamo come output:

\lstinputlisting[language={[5.0]Lua}]{snippets/gmatch_english_output.lua}

che \`e esattamente ci\`o che ci si aspetterebbe.

Tuttavia, eseguendo questo codice

\begin{samepage}
\lstset{inputencoding=utf8/latin1}
\lstinputlisting[language={[5.0]Lua}]{snippets/gmatch_italian.lua}
\end{samepage}

otteniamo come output

\begin{minipage}{\textwidth}
\lstset{inputencoding=utf8/latin1}
\lstinputlisting[language={[5.0]Lua}]{snippets/gmatch_italian_output.lua}
\end{minipage}

al posto delle lettere accentate di cui \`e composta la stringa.
Inoltre, nonostante la stringa sia composta da 5 caratteri, l'iteratore ne ha restituiti 10.

Tutto ci\`o \`e dovuto al fatto che lua \`e, come viene definito nella documentazione ufficiale, unicode-agnostic.
Questo significa che lua tratta tutte le stringhe come una sequenza arbitraria di byte, quindi pu\`o contenere qualsiasi carattere in qualsiasi codifica, utf8 compresa.
Tuttavia la maggior parte delle funzioni che lua mette a disposizione per manipolare le stringhe, compresa la funzione \emph{gmatch}, trattano queste ultime come sequenze di byte, mentre utf8 prevede l'uso di 2 byte per codificare i caratteri accentati.

La stringa ``\`a\`e\`i\`o\`u'' \`e quindi composta da 10 byte, per questo motivo l'iteratore restituito da \emph{gmatch} trova 10 caratteri invece di 5, che, per di pi\`u, sono caratteri non stampabili.

Questo problema \`e stato risolto utilizzando la libreria \emph{luautf8}, che mette a disposizione implementazioni delle funzioni di manipolazioni di stringhe compatibili con la codifica utf8.

\subsubsection{nn}
La libreria \emph{nn} mette a disposizione una serie di classi con le quali \`e possibile costruire ed addestrare reti neurali, di qualsiasi complessit\`a, in maniera modulare.
Queste possono essere suddivise in tre macro categorie: \emph{moduli}, \emph{contenitori} e \emph{criteri}.

I \emph{moduli} costituiscono i mattoni con cui \`e possibile costruire reti neurali.
Ognuno di essi espone i metodi necessari per definire un un livello della rete neurale e fra questi \`e possibile individuarne due di fondamentale importanza (per una descrizione degli altri metodi, rimando alla documentazione ufficiale):

\begin{itemize}
  \item il metodo \emph{forward(input)} che prende in pasto i dati di input e calcola i corrispondenti dati di output
  \item il metodo \emph{backward(input, gradOutput)} che effettua un passaggio di retro propagazione attraverso il modulo, in base ai dati di input forniti
\end{itemize}

Ciascun modulo \`e, a sua volta, una rete neurale e pu\`o essere combinato con altri moduli per creare reti neurali pi\`u complesse, tramite l'uso di \emph{contenitori}.
Questi ultimi sono simili ai moduli, e infatti espongono gli stessi metodi esposti dai moduli, tuttavia il loro scopo non \`e quello di applicare una trasformazione ai dati di input, bens\`i di orchestrare le interazioni fra i moduli contenuti.
Metodi \emph{forward(input)} e \emph{backward(input, gradOutput)} di un contenitore, infatti, non lavorano direttamente sui dati ma si occupano di chiamare i rispettivi metodi dei moduli contienuti con modalit\`a che dipendono dalla particolare implementazione del contenitore.

Esistono diverse implementazioni di moduli e contenitori, nella libreria \emph{nn}, con le quali \`e possibile costruire reti neurali complesse a piacere.

Per descriverne l'utilizzo consideriamo il seguente esempio:
\lstinputlisting[language={[5.0]Lua}]{snippets/linear.lua}

Questa porzione di codice crea una semplice rete neurale di tipo \emph{feed-forward} con un solo livello, utilizzando un modulo di tipo \texttt{Linear} ed un contenitore di tipo \texttt{Sequential}.

Nella prima riga viene creato il modulo \texttt{nn.Linear}, che rappresenta l'unico livello della rete.
Questo prende in input due parametri, rispettivamente il numero di dati in intput e il numero di dati in output, e restituisce un modulo il cui metodo \texttt{forward} calcola l'output applicando una trasformazione lineare ai dati di input (es. $\vec{y} = \vec{a}\vec{x} + b$ )

Nella seconda e terza riga viene creato il contenitore \texttt{nn.Sequential} e a quest'ultimo viene aggiunto il modulo \texttt{nn.Linear} appena creato.
Il metodo \texttt{forward} del contenitore \texttt{nn.Sequetial} non fa altro che chiamare i metodi \texttt{forward} dei moduli contenuti, che rappresentano i livelli della rete, in maniera sequenziale e rispettando l'ordine con cui sono stati aggiunti.
L'output del modulo $l_n$ costituisce l'input del modulo $l_{n+1}$.

Una propagazione in avanti \`e eseguita in questo modo:

\lstinputlisting[language={[5.0]Lua}]{snippets/linear_forward.lua}

Mentre, per effettuare una retro propagazione:

\lstinputlisting[language={[5.0]Lua}]{snippets/linear_backward.lua}

La libreria mette a disposizione molti altri moduli, come ad esempio:

\begin{itemize}
    \item \emph{Identity}: restituisce in output qualsiasi valore passato in input, senza applicare alcuna trasformazione. Utile per creare il livello di input di una rete neurale.
    \item \emph{Add}: aggiunge un valore di distorsione ai dati di input, es. $y_i = x_i + b_i$ oppure, se $scalar = true$, utilizza un unico valore di distorsione $y_i = x_i + b$.
    \item \emph{Mul}: moltiplica i dati di input per uno scalare $w$, es. $y = wx$
\end{itemize}

Infine abbiamo i \emph{criteri}, che sono fondamentali per l'apprendimento di una rete neurale.
Quest'ultimi, infatti, sono usati per calcolare i gradienti, dati input e output, in base ad una determinata funzione di perdita.

Possono essere raggruppati in:
\begin{itemize}
    \item \emph{Classification}
    \item \emph{Regression}
    \item \emph{Embedding criterions}
    \item \emph{Miscelaneous criterions}
\end{itemize}

Come qualsiasi altro modulo, anche i criteri espongono i metodi:
\begin{itemize}
  \item \texttt{forward(predicted, target)} dati i valori calcolati dalla rete neurale (\texttt{predicted}) e quelli reali, obiettivo dell'apprendimento, calcola la perdita in base alla funzione di perdita associata al criterio utilizzato.
  \item \texttt{backward(input, target)} dati input e target calcola il gradiente in base alla funzione di perdita associata al criterio.
\end{itemize}

Queste funzioni sono usate per calcolare la perdita e aggiornare di conseguenza i pesi associati a ciascun arco della rete neurale, durante la fase di addestramento.

\subsubsection{nngraph}
Un estensione della libreria \emph{nn} che semplifica la creazione di architetture neurali complesse.

\subsubsection{optim}
Questa libreria implementa parecchi algoritmi di ottimizzazione che possono essere utilizzati per addestrare una rete neurale.

Una rete neurale pu\`o essere addestrata usando un semplice ciclo \texttt{for} e una \emph{funzione d'apprendimento}.
Nella seguente porzione di codice la funzione \texttt{gradientUpgrade} una singola fase di apprendimento, che consiste in una propagazione in avanti seguita da una retro propagazione e un conseguente aggiornamento dei pesi attribuiti agli archi della rete.

\lstinputlisting[language={[5.0]Lua}]{snippets/for_loop_learning.lua}

Tuttavia, la librearia \texttt{Optim}, semplifica il processo di apprendimento mettendo a disposizione una lista completa di algoritmi di ottimizzazione gi\`a implementati, che possono essere utilizzati per addestrare una rete neurale.

Fra questi troviamo:

\begin{itemize}
  \item \emph{adadelta}
  \item \emph{adagrad}
  \item \emph{adam}
  \item \emph{asgd}
  \item \emph{fista}
  \item \emph{lbfgs}
  \item \emph{lswolfe}
  \item \emph{rmsprop}
  \item \emph{rprop}
  \item \emph{sgd}
\end{itemize}

Tutti i metodi condividono la stessa interfaccia:

\lstinputlisting[language={[5.0]Lua}]{snippets/optim_interface.lua}

Di seguito, un esempio di come utilizzare la libreria \texttt{optim} per addestrare una rete neurale:

\lstinputlisting[language={[5.0]Lua}]{snippets/optim_example.lua}

\subsubsection{GPGPU}
La \emph{GPGPU}, sigla di \emph{general-purpose computing on graphics processing units} (letteralmente ``calcolo a scopo generale su unit\`a di elaborazione grafiche'') \`e un settore della ricerca informatica che ha come scopo l'utilizzo della GPU per scopi diversi dalla tradizionale creazione di un'immagine tridimensionale; in tale ambito di utilizzo la GPU viene impiegata per elaborazioni estremamente esigenti in termini di potenza di elaborazione, e per le quali le tradizionali architetture di CPU non hanno una capacit\`a di elaborazione sufficiente.

Tale tipo di elaborazioni sono, per loro natura, di tipo altamente parallelo, e in grado quindi di beneficiare ampiamente dell'architettura tipica delle GPU. A tale caratteristica intrinseca, a partire dal 2007 si \`e aggiunta l'estrema programmabilit\`a offerta dalle ultime soluzioni commerciali, che al succedersi delle generazioni aumentano non solo la propria potenza elaborativa ma anche la propria versatilit\`a. Tale evoluzione delle architetture video si sta gradualmente attuando anche perch\`e la stessa grafica 3D dei videogiochi pi\`u recenti si \`e enormemente trasformata nel tempo; un tempo essi erano basati su un insieme di istruzioni fisse e predefinite, ma progressivamente si sta sviluppando un approccio nel quale le GPU vengono completamente programmate utilizzando gli shader, caratteristica che apre di conseguenza anche nuovi scenari di utilizzo a questo tipo di soluzioni hardware. Tale programmabilit\`a ha preso appunto il nome di Shader model.

\`E evidente che le applicazioni che sono in grado di avvantaggiarsi significativamente della potenza di calcolo delle moderne GPU sono solo un sottoinsieme dell'intero panorama software, in quanto per sfruttare le caratteristiche di tali architetture \`e necessaria un'elevata parallelizzazione del codice, una caratteristica tipica di alcuni problemi scientifici.

L'algoritmo di addestramento di una rete neurale rientra perfettamente in questo sottoinsieme, in quanto altamente paralelizzabile.
Infatti, per ciascun livello, i calcoli necessari ad eseguire una propagazione di un singolo nodo sono indipendenti da quelli eseguiti per altri nodi dello stesso livello, quindi possono essere eseguiti in parallelo.

Al momento sono due le principali piattaforme che permettono di effettuare calcoli general-purpose su GPU: \emph{CUDA} e \emph{OpenCL}.

CUDA \`e l'architettura di elaborazione in parallelo di NVIDIA.
Essa permette di sfruttare le GPU compatibli per eseguire processi general-purpose, tuttavia si tratta di una proposta propietaria e, attualmente, gli unici dispositivi compatibili sono le GPU \emph{GeForce}, \emph{ION}, \emph{Quadro} e \emph{Tesla} di NVIDIA.

OpenCL \`e un framework basato sul linguaggio ANSI C e C++ che permette di scrivere software che pu\`o essere eseguito su una molteplicit\`a di piattaforme, comprese CPU e GPU.
Si tratta di un progetto open source, originariamente proposto dalla Apple, successivamente ratificato dalla stessa assieme alle principali aziende del settore (Intel, NVIDIA, AMD), e infine portato a compimento dal consorzio no-profit Khronos Group.
Essendo uno standard aperto \`e compatibile con una variet\`a di dispositivi, comprese le GPU AMD, Intel ed NVIDIA.

Torch7 \`e compatibile con entrambe le piattaforme, mettendo a disosizione le librerie \texttt{cutorch} e \texttt{cunn} per CUDA, \texttt{cltorch} e \texttt{clnn} per OpenCl.

\lstinputlisting[language={bash}]{snippets/luarocks_cuda.sh}

Per il supporto alla piattaforma CUDA.

\lstinputlisting[language={bash}]{snippets/luarocks_cltorch.sh}

Per il supporto al framework OpenCL.
